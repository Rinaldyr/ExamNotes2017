<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title> Computational Vision </title>
    <style type="text/css">
        body {
            background-color: #ffe6b3;
            font-family: "Georgia", serif;
        }

        #topBtn {
            display: none;
            position: fixed;
            bottom: 80px;
            right: 20px;
            z-index: 99;
            border: none;
            outline: none;
            background-color: red;
            color: white;
            cursor: pointer;
            padding: 15px;
            border-radius: 10px;
        }

        #topBtn:hover {
            background-color: #555;
        }

        #homeBtn {
            display: block;
            position: fixed;
            bottom: 20px;
            right: 20px;
            z-index: 99;
            border: none;
            outline: none;
            background-color: red;
            color: white;
            cursor: pointer;
            padding: 15px;
            border-radius:10px;
        }

        #homeBtn:hover {
            background-color: #555;
        }

        table, tr, td {
            border: 1px solid #000000;
            border-collapse: collapse;
        }

        td {
            text-align: center;
            width: 15px;
        }

        table {
            margin: 5px;
        }
    </style>
</head>
<body>
<hr/>
    <h1> LI Computational Vision </h1>
<hr/>
Table of contents
<ol>
    <li><a href="#intro"> Introduction </a></li>
    <li><a href="#human"> Human Vision </a></li>
    <li><a href="#edge"> Edge Detection </a></li>
    <li><a href="#noise"> Noise Filtering </a></li>
    <li><a href="#colour"> Colour </a></li>
    <li><a href="#adv"> Advanced Edge Detection </a></li>
    <li><a href="#hough"> Hough Transform </a></li>
    <li><a href="#sift"> SIFT - Scale Invariant Feature Transform </a></li>
    <li><a href="#face"> Face Recognition </a></li>
    <li><a href="#motion"> Motion </a></li>
    <li><a href="#roc"> ROC Analysis </a></li>
    <li><a href="#object"> Object Recognition </a></li>
    <li><a href="#model"> Model Based Object Recognition </a></li>
</ol>
<hr/>

<h2 id="intro"> 1 - Introduction </h2>
<p> What is Computational Vision? Vision is the process of discovering from images what is present in the world,
    and where it is.</p>
<p align="center"> <mark><strong> The acquisition of knowledge <br></strong></mark>
    <span style="color:#FF0000"> about </span><mark><strong><br> objects and events in the environment
        <br></strong></mark><span style="color:#FF0000"> through </span><mark><strong><br> information processing
        <br></strong></mark><span style="color:#FF0000"> of </span><mark><strong><br>
        light emitted or reflected from objects </strong></mark></p>
<p>To make computers 'see'</p>
<p>"Automatic inference" of "properties" of "the world" from "images"</p>
<ul>
    <li><strong>Automatic inference</strong>
        <ul>
            <li>Inference without (or minimal) human intervention</li>
        </ul></li>
    <li><strong>The world</strong>
        <ul>
            <li>The real unconstrained 3D physical world</li>
            <li>Constrained/Engineered environments</li>
        </ul></li>
    <li><strong>Image</strong>
        <ul>
            <li>2D projection of the electromagnetic signal provided by the world</li>
        </ul></li>
    <li><strong>Properties</strong>
        <ul>
            <li>Geometric: shape, size, location, distance</li>
            <li>Material : color, texture, reflectivity, transparency</li>
            <li>Temporal: direction of motion (in 3D), speed, events</li>
            <li>Illumination: light source specification, light source color</li>
            <li>Symbolic: objects' class, object's ID</li>
        </ul></li>
</ul>
<hr/>

<h2 id="human"> 2 - Human Vision </h2>
<p><strong>Evolution of eyes:</strong></p>
<ul>
    <li>Single cell - 1D capture of light</li>
    <li>Multiple cell - Better direction resolution</li>
    <li>A pinhole camera dilemma:
    <ul>
        <li>Wide aperture:<ul>
            <li>Bright images</li>
            <li>Fuzzy images</li>
        </ul></li>
        <li>Pinhole aperture:<ul>
            <li>Dim images</li>
            <li>Sharp images</li>
        </ul></li>
    </ul></li>
</ul>
<p> <strong>Solution:</strong> Use of <strong>light refraction</strong> and hence <strong>lenses</strong></p>
<p> <strong>Refraction</strong> (Snell's Law)<br><em>The wavelength changes, but wave crests can't be created or destroyed
at the interface, so to make the waves match up, the light has to change <strong>direction.</strong></em></p>
<p><strong>Pinhole Camera:</strong></p>
<ul>
    <li>Basic geometry</li> <img src="images/vision/pinholeCamera.png" width="400">
    <li>Perspective projection</li> <img src="images/vision/perspectiveProjection.png" width="400">
</ul>
<p><strong>Image formation</strong></p>
<p style="text-align: center;"><img src="images/vision/formation.png" width="300" align="center"></p>
<ul>
    <li>f = the focal length (metres)</li>
    <li>1/f = the power of the lens (dioptres)</li>
    <li>Human eye has power of about 59 dioptres <ul>
        <li>1/f = 50 dioptres</li>
        <li>f = 1/50 = 0.02m</li>
    </ul></li>
</ul>
<p style="text-align: center;"><img src="images/vision/formation2.png" width="300"></p>
<ul>
    <li>As an object moves closer, the power of the lens must increase to accommodate</li>
    <li>So if the object is infinitely far away <em><sup>1</sup>&frasl;<sub>f</sub> = <sup>1</sup>&frasl;<sub>&infin;</sub>
        + <sup>1</sup>&frasl;<sub>0.02</sub> = 50 dioptres</em></li>
    <li>But if it is 1m away the lens must change shape to produce a sharp image <em><sup>1</sup>&frasl;<sub>f</sub>
        = <sup>1</sup>&frasl;<sub>1</sub> + <sup>1</sup>&frasl;<sub>0.02</sub> = 51 dioptres</em></li>
</ul>
<p> The retina contains two types of <strong>photoreceptors</strong> that respond to light</p>
<ul>
    <li>Rods<ul>
        <li>&approx;120million</li>
        <li>Extremely sensitive</li>
        <li>Respond to single photon</li>
        <li>Poor spatial resolution as they converge to same neuron within retina</li>
    </ul></li>
    <li>Cones<ul>
        <li>&approx;6million</li>
        <li>Active at higher light levels</li>
        <li>Higher resolution as Signal processed by several neurons</li>
    </ul></li>
</ul>
<p><strong>2 types of Ganglion cells</strong></p>
<ul>
    <li>On-center: stimulated when the center of its receptive field is exposed to light, inhibited when the surround
    is exposed to light</li>
    <li>Off-center: opposite reaction</li>
</ul>
<p style="text-align: center;"><img src="images/vision/ganglion.png" width="240"></p>
<ul>
    <li>Allows ganglion cells to also transmit information about difference in
        firing rates of cells in the center and surround</li>
    <li>Allows transmission of information about contrast</li>
    <li>The size of the receptive field governs the spatial frequency of the information:<ul>
        <li>small receptive fields are stimulated by high spatial frequencies, fine detail</li>
        <li>large receptive fields are stimulated by low spatial frequencies, coarse detail</li>
        <li>Retinal ganglion cell receptive fields convey information about discontinuities
            in the distribution of light falling on the retina; these often specify the edges of objects</li>
    </ul></li>
</ul>
<p><strong>Visual Pathway</strong></p>
<p style="text-align: center;"><img src="images/vision/pathway.png" width="300"></p>
<ol>
    <li>Vision generated by photoreceptors in retina</li>
    <li>Information leaves eye through optic nerve</li>
    <li>There is a partial crossing of axons at the optic chiasm</li>
    <li>After the chiasm, the axons are called the optic tract</li>
    <li>Optic tract wraps around the midbrain to get to the lateral geniculate nucleus (LGN)</li>
    <li>LGN axons travel to primary visual cortex at the back of the brain</li>
</ol>

<hr/>

<h2 id="edge"> 3 - Edge Detection </h2>
<p>Intensity Images</p>
<ul>
    <li>Data matrix where values represent intensities</li>
    <li>Single matrix, with each element corresponding to one image pixel</li>
</ul>
<p><strong>Indexed Images</strong></p>
<ul>
    <li>Consists of data matrix and a colormap matrix</li>
    <li>Map is an m-by-3 array of doubles containing floating point values between 0 and 1</li>
    <li>Each row of map specifies RGB components of a single color</li>
</ul>
<p><strong>Intensity gradients</strong></p>
<ul>
    <li>Image is a function that maps coordinates to intensity - <em>f(x,y)</em></li>
    <li>Gradient of intensity is a vector with x and y components</li>
</ul>
<p><strong>Approximating the gradient</strong></p>
<ul>
    <li>We use a 2x2 mask<br>
    <table style="float: left">
        <caption>G<sub>x</sub></caption>
        <tr>
            <td>-1</td>
            <td>1</td>
        </tr>
        <tr>
            <td>-1</td>
            <td>1</td>
        </tr>
    </table>
    <table>
        <caption>G<sub>y</sub></caption>
        <tr>
            <td>1</td>
            <td>1</td>
        </tr>
        <tr>
            <td>-1</td>
            <td>-1</td>
        </tr>
    </table></li>
    <li>For each mask of weights you multiply the corresponding pixel by the weight and sum over all pixels</li>
</ul>
<p>Other edge detectors</p>
<ul>
    <li><strong>Roberts</strong><br>
        <table style="float: left">
            <caption>G<sub>x</sub></caption>
            <tr>
                <td>1</td>
                <td>0</td>
            </tr>
            <tr>
                <td>0</td>
                <td>-1</td>
            </tr>
        </table>
        <table>
            <caption>G<sub>y</sub></caption>
            <tr>
                <td>0</td>
                <td>-1</td>
            </tr>
            <tr>
                <td>1</td>
                <td>0</td>
            </tr>
        </table>
    </li>
    <br>
    <li><strong>Sobel</strong><br>
        <table style="float: left">
            <caption>G<sub>x</sub></caption>
            <tr>
                <td>-1</td>
                <td>0</td>
                <td>1</td>
            </tr>
            <tr>
                <td>-2</td>
                <td>0</td>
                <td>2</td>
            </tr>
            <tr>
                <td>-1</td>
                <td>0</td>
                <td>1</td>
            </tr>
        </table>
        <table>
            <caption>G<sub>y</sub></caption>
            <tr>
                <td>1</td>
                <td>2</td>
                <td>1</td>
            </tr>
            <tr>
                <td>0</td>
                <td>0</td>
                <td>0</td>
            </tr>
            <tr>
                <td>-1</td>
                <td>-2</td>
                <td>-1</td>
            </tr>
        </table>
    </li>
</ul>
<p><strong>Convolution</strong></p>
<ul>
    <li>Computation of weighted sums of image pixels</li>
    <li>For each pixel [i,j], the value of h[i,j] is calculated by <strong>translating</strong> the mask
    to pixel [i,j] and taking the weighted sum of pixels in neighbourhood</li>
</ul>
<p>Steps:</p>
<ul>
    <li>Take image</li>
    <li>Convolve mask with image for each direction<ul>
        <li>Calculate G<sub>x</sub> and G<sub>y</sub></li>
    </ul></li>
    <li>Calculate magnitude (using Pythagoras' theorem)</li>
</ul>
<p><strong>Filtering</strong></p>
<ul>
    <li>We can detect edges by calculating intensity change (gradient) across image</li>
    <li>This is implemented using the idea of filtering</li>
</ul>
<hr/>

<h2 id="noise"> 4 - Noise Filtering </h2>
<p><strong>Noise</strong></p>
<ul>
    <li>We need to remove noise</li>
    <li>There are many noise filters</li>
    <li>Most can be implemented using convolution</li>
    <li>e.g. Mean filter - this is a linear filter</li>
    <li>The most widely used is <strong>Gaussian filtering</strong></li>
</ul>
<p><strong>Sequenced filters</strong></p>
<ul>
    <li>We can replace a 2D Gaussian filter with 2, 1D Gaussian filters in sequence</li>
    <li>Efficiency and performance</li>
</ul>
<p><em>Reading: Laplacian, Laplacian of Gaussian, Gaussian (Canny) Edge detection and Thresholding</em></p>
<hr/>

<h2 id="colour"> 5 - Colour </h2>
<ul>
    <li>Objects selectively absorb some wavelengths and absorb others</li>
    <li>Human retinas contain 3 different kinds of cones to give us the ability to distinguish different
        forms of same objects</li>
</ul>
<p><strong>Colour mixing</strong></p>
<ul>
    <li><strong>Trichromatic (three colour) theory: </strong>eye's 3 different types of receptors sensitive
    to different hues. Any colour can be produced by appropriate mixing of the 3 primary colours</li>
    <li>Retina contains approximately equal numbers of red and green cones, but only 8% of blue</li>
    <li>Theory explains:<ul>
        <li>How we discriminate wavelengths 2nm in difference</li>
        <li>How we can match a mixture of wavelengths to a single colour</li>
        <li>Some types of colour blindness</li>
    </ul></li>
    <li>Does <strong>NOT</strong> account for colour blending<ul>
        <li>Some colour blend while others don't</li>
    </ul></li>
    <li>Primary colours: Yellow, Blue, Red and Green<ul>
        <li>Trichromatic theory cannot explain why yellow is a primary colour</li>
    </ul></li>
</ul>
<p><strong>Opponent Process Coding</strong></p>
<ul>
    <li>Neurons respond to <strong>pairs</strong> of primary colours</li>
    <li>Some respond in centre-surround fashion</li>
    <li>Response characteristics determines by appropriate ganglion cells connections<br>
    <img src="images/vision/colorganglion.png" width="300"></li>
</ul>
<hr/>

<h2 id="adv"> 6 - Advanced Edge Detection </h2>
<p>What causes intensity changes?</p>
<ul>
    <li>Geometric events<ul>
        <li>surface orientation (boundary) discontinuities</li>
        <li>depth discontinuities</li>
        <li>colour and texture discontinuities</li>
    </ul></li><br>
    <li>Non-geometric events<ul>
        <li>illumination changes</li>
        <li>specularities</li>
        <li>shadows</li>
        <li>inter-reflections</li>
    </ul><br><img src="images/vision/causes.png" width="300"></li>
</ul>
<p><strong>Edge Descriptors</strong></p>
<ul>
    <li><strong>Edge direction</strong><ul>
        <li>perpendicular to the direction of maximum intensity change (i.e. edge normal)</li>
    </ul></li>
    <li><strong>Edge strength</strong><ul>
        <li>related to the local image contrast along the normal</li>
    </ul></li>
    <li><strong>Edge position</strong><ul>
        <li>the image position at which the edge is located</li>
    </ul><br><img src="images/vision/descriptors.png" width="200"></li>
</ul>
<p><strong>Main Steps of Edge Detection</strong></p>
<ol>
    <li><strong>Smoothing</strong><ul>
        <li>suppress as much noise as possible, without destroying true edges</li>
    </ul></li>
    <li><strong>Enhancement</strong><ul>
        <li>apply differentiation to enhance the quality of edges (i.e. sharpening)</li>
    </ul></li>
    <li><strong>Thresholding</strong><ul>
        <li>determine which edge pixels should be discarded as noise and which should be kept</li>
    </ul></li>
    <li><strong>Localisation</strong><ul>
        <li>determine the exact edge location</li>
        <li><em>sub-pixel resolution might be required for some applications to estimate
        the location of an edge to better than spacing between pixels</em></li>
    </ul></li>
</ol>
<p><strong>Edge Detection Using Derivatives</strong></p>
<ul>
    <li>Often points that lie on an edge are detected by:<ol>
        <li>Detecting the local <strong>maxima</strong> or <strong>minima</strong> of 1st derivative</li>
        <li>Detecting the <strong>zero-crossings</strong> of the 2nd derivative</li>
    </ol> <br><img src="images/vision/derivatives.png" width="200"></li>
</ul>
<p><strong>Practical Issues</strong></p>
<ul>
    <li>Noise suppression-localisation tradeoff<ul>
        <li>Smoothing depends on mask size</li>
        <li>Larger mask sizes reduce noise, but worsen localisation (i.e. add uncertainty to the location
        of the edge) and vice versa</li>
    </ul></li>
    <li>Choice of threshold<br><img src="images/vision/thresholding.png" width="500"></li>
</ul>
<p><strong>Criteria for Optimal Edge Detection</strong></p>
<ol>
    <li><strong>Good detection</strong><ul>
        <li><strong>Minimise</strong> the probability of <strong>false positives</strong> (i.e. spurious
            edges)</li>
        <li><strong>Minimise</strong> the probability of <strong>false negatives</strong> (i.e. missing
        real edges)</li>
    </ul></li>
    <li><strong>Good localisation</strong><ul>
        <li>Detected edges must be as close as possible to the true edges</li>
    </ul></li>
    <li><strong>Single response</strong><ul>
        <li>Minimise the number of local maxima around the true edge</li>
    </ul></li>
</ol>
<p><strong>Canny edge detector</strong></p>
<ul>
    <li>Canny has shown that the <strong>first derivative of the Gaussian</strong> closely approximates
    the operator that optimises the product of <em>signal-to-noise ratio</em> and <em>localisation
        </em></li>
</ul>
<p><strong>Hysteresis Thresholding</strong></p>
<ul>
    <li>Standard thresholding<ul>
        <li>Can only select "strong" edges</li>
        <li>Does not guarantee "continuity</li>
    </ul></li>
    <li>Hysteresis thresholding uses two thresholds:<ul>
        <li>low threshold - t<sub>l</sub></li>
        <li>high threshold - t<sub>h</sub> (usually t<sub>h</sub> = 2t<sub>l</sub>)</li>
    </ul></li>
    <li>Making the assumption that important edges should be along continuous curves in the image allows us to
        follow a faint section of a given line and to discard a few noisy pixels that do not constitute a line
        but have produced large gradients.</li>
    <li>We begin by applying a high threshold. This marks out the edges we can be fairly sure are genuine.</li>
    <li>Starting from these, using the directional information derived earlier,
        edges can be traced through the image.</li>
    <li>While tracing an edge, we apply the lower threshold, allowing us to trace faint
        sections of edges as long as we find a starting point.</li>
</ul>
<hr/>

<h2 id="hough"> 7 - Hough Transform </h2>
<p>So far we have only found edge points, not edge segments<br>
The Hough transform is a common approach to finding parameterised line segments</p>
<p>The basic idea:</p>
<p align="center"><img src="images/vision/hough1.png" width="150"></p>
<ul>
    <li>Each <strong>straight line</strong> in this image can be described by an <strong>equation</strong></li>
    <li>Each <strong>white point</strong>, if considered in isolation, could lie on an <strong>infinite</strong>
    number of <strong>straight lines</strong></li>
</ul>
<p align="center"><img src="images/vision/hough2.png" width="150"></p>
<ul>
    <li>In the Hough transform each point <strong>votes</strong> for every line it could be on</li>
    <li>The lines with the <strong>most votes</strong> win</li>
</ul>
<p align="center"><img src="images/vision/hough3.png" width="150"></p>
<ul>
    <li>Any line can be represented by two numbers</li>
    <li>The yellow line will be represented by (w,&Phi;)<br>In other wods we define it using:<ul>
        <li>a line from an agreed origin</li>
        <li>of length w</li>
        <li>at angle &Phi; to the horizontal</li>
    </ul></li>
</ul>
<p align="center"><img src="images/vision/hough4.png" width="150"></p>
<ul>
    <li>
        So we can represent any line in the image space as a point in the plane defined by (w,&Phi;)
    </li>
    <li>This is called <strong>Hough Space</strong></li>
</ul>
<p align="center"><img src="images/vision/hough5.png" width="350"></p>
<ul>
    <li>One point in image space corresponds to a sinusoidal curve in houghspace</li>
    <li>Two points correspond to two curves in Hough space</li>
    <li>The intersection of those two curves has "two votes"</li>
    <li>This intersection represents the straight line in image space that passes through both points</li>
</ul>
<p align="center"><img src="images/vision/hough6.png" width="250"></p>
<ul>
    <li>There are generalised versions for ellipses, circles</li>
    <li>For the straight line transform we need to supress <strong>non-local maxima</strong></li>
    <li>The input image could also benefit from <strong>edge thinning</strong></li>
    <li>Single line segments not isolated</li>
    <li>Will still fail in the face of certain textures</li>
</ul>
<p><strong>Circle Hough Transform</strong></p>
<ul>
    <li>Can be used to determine parameters of a circle when a number of points that fall on perimeter are known</li>
    <li>A circle with radius R and center (a,b) can be described with the following parametric equations:<ul>
        <li><em>x = a + Rcos(&theta;)</em></li>
        <li><em>y = b + Rsin(&theta;)</em></li>
    </ul></li>
    <li>When &theta; sweeps through the full 360 degree range, the points (x,y) trace the perimeter of the circle</li>
    <li>If an image contains many points, some of which are on the perimeters of circles, then the job
    of the search program is to find parameter triplets (a,b,R) to describe each circle</li>
    <li>The fact that the parameter space is 3D makes a direct implementation of the Hough technique
    more expensive in computer memory and time</li>
    <li>Examples:</li>
</ul>
<p align="center"><img src="images/vision/hough7.png" width="500"></p>
<hr/>

<h2 id="sift"> 8 - SIFT - Scale Invariant Feature Transform </h2>
<p><em>Why do we care about matching features?</em></p>
<ul>
    <li>Object Recognition</li>
    <li>Wide baseline matching<ul>
        <li>Given any two images, estimate the fundamental matrix and a set of matched interest points</li>
    </ul></li>
    <li>Tracking</li>
</ul>
<p><strong>We want Invariance!!!</strong></p>
<ul>
    <li>Good features should be robust to all sorts of nastiness that can occur between images</li>
</ul>
<p><strong>Types of invariance:</strong></p>
<ul>
    <li>Illumination</li>
    <li>Scale</li>
    <li>Rotation</li>
    <li>Affine</li>
    <li>Full Perspective</li>
</ul>
<p>How to achieve illumination invariance</p>
<ul>
    <li>Normalising (easy way)</li>
    <li>Difference based metrics (sift)</li>
</ul>
<p>How to achieve scale invariance</p>
<ul>
    <li>Pyramids<ul>
        <li>Divide width and height by 2</li>
        <li>Take average of 4 pixels for each pixel(or Gaussian blur)</li>
        <li>Repeat until image is tiny</li>
        <li>Run filter over each size image and hope its robust</li>
    </ul></li>
    <li>Scale Space (DOG method)<ul>
        <li>Pyramids but fill gaps with blurred images</li>
        <li>Like having a nice linear scaling without the expense</li>
        <li>Take features from difference of these images</li>
        <li>If the feature is repeatably present in between Difference of Gaussians it is Scale Invariant
        and we should keep it</li>
    </ul></li>
</ul>
<p>How to achieve rotation invariance</p>
<ul>
    <li>Rotate all features to go the same way in a determined manner</li>
    <li>Take histogram of Gradient directions</li>
    <li>Rotate to most dominant</li>
</ul>
<hr/>

<h2 id="face"> 9 - Face Recognition </h2>
<hr/>

<h2 id="motion"> 10 - Motion </h2>
<hr/>

<h2 id="roc"> 11 - ROC Analysis </h2>
<hr/>

<h2 id="object"> 12 - Object Recognition </h2>
<hr/>

<h2 id="model"> 13 - Model Based Object Recognition </h2>

    <!-- Utils -->
    <button onclick="topFunction()" id="topBtn" title="Go to top">Top</button>
    <script>
        // When the user scrolls down 20px from the top of the document, show the button
        window.onscroll = function() {scrollFunction()};

        function scrollFunction() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                document.getElementById("topBtn").style.display = "block";
            } else {
                document.getElementById("topBtn").style.display = "none";
            }
        }

        // When the user clicks on the button, scroll to the top of the document
        function topFunction() {
            document.body.scrollTop = 0;
            document.documentElement.scrollTop = 0;
        }
    </script>
    <button onclick="window.location.href='index.html'" id="homeBtn" title="Back to Home"> Home </button>
</body>
</html>
